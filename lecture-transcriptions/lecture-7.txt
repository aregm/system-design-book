 For link. Yes. So let's talk today's lecture is continuation of the journey that we are. We are all on and now we are going to discuss the role of algorithm and how to move from algorithm to a system and that it's not quite straightforward how to do that and the role or system design in making algorithm run, especially on a large scale and. And we are going to study that. Is one of the example using Pagerank. Algorithm and Google search engine. Conceptual design of a Google search engine. Of course, it's not the detailed design. So yeah, I'm going to talk about what is the algorithm. The algorithm is not enough levels of detailization of the algorithm and how it's going to affect the system. Design decisions. And running the bump that scale. So. You know guys that web is can be represented as a very, very, very large graph where nodes are the web pages and the edges are the hyper links from one page to another. So these lectures have all based on. Excellent book run written by all Man and his colleagues called Manning of Massive data sets and a series of lecture in Stanford. Course CS2469. There's a link here you can follow up with later, and there's a specialized course about search engines design at the University of Waterloo. I'm acia 541, so let's dive in. Yes, as we talked web as a directed graph. So I'm going to go quite quickly. Please stop at that point. Any point? We have lots of material to cover. What if you have any questions? OK, this is important that you stop me at some point, I will stop and ask questions. Yeah. In the early days of the early days of the Internet, there was very, very. Actual and problem big problem. How do organized web? Well, if you enter to the connect to the Internet, that was a thing back in 90s. Where should you start? And that's where. Web directories. But Yahoo came on and. It was. Directory like Yellow Pages. I don't know whether you guys know the Yellow Pages was just a directory. Combined or ordered by topics. Of web pages at the Internet. Where in Yahoo there were not engineers, but employees were surfing web. Selecting those pages and adding them to this document. Very quickly it was obvious that this is cannot be done by humans. We need some type of. Automated system and that's where search engines came in. So now let's. Think that. Let's imagine you're working at Yahoo. What are challenges with the search system like? What are the problems that you need to solve to make this web search an efficient, efficient tool for users? What do you think? No, they might type the thing that they want to search, but. They don't mean it. How do I say it? They might type some things that they think the search might recognize, but they, but it doesn't. The search might what? Sorry. So when someone thinks the web should recognize what they want to. But the concept is in their mind and they type by letters and other thing. IPod, I would say that one of the two, as we see on the Slide 2 main things. One is what's the best answer to your question, right? You enter some term. Doesn't matter football. What is the best answer to give you? And the second whom you need to trust. Right. If we OK here we. Go because like football or any type of newspaper in this case like. How do you sort the output? To be relevant and the 2nd. If you had, if you hit some interesting term or you want to include the result into your output, can you trust it? So that is the problem of ranking, because not every web page has equal importance quarter. Some Web blog about chocolate is not as important As for example this case, Stanford. Edu if you enter search engine search phrase like computer science education. So. Based on this, the first problem which we need to solve is based on these large graph. Of web pages and links, how we are going to rank? Pages, even this structure and this is a quite. Challenging problem. And let's think, OK. Oops. Yeah. Let's assume you encountered this problem first time. What do you think? What? How we are going to solve this? There's a of course there is a hint here, but. How do you want to include? How do you want to? For this import. Sorry for this importance. No. Good. I think each page might have. Value that has the importance by a number, and if that important here the web page OK. shows higher. Yes. So that's the original idea behind the page rack. Before page rank. So there were two algorithms that came together. Around the same time, 909819971998, I think 98. I don't remember correctly. One was called hits and the second one was called page rank and they both algorithms were trying to solve. No problem. How to rank the output based on some notion of importance? The consequent problems that needs to be solved related to the page run like how do you personalize this page, right? Because football for me is one thing. Football for you can be a different thing and you need different types of ranking also. There's lots of noise in the web. How do you filter out spec? And for that in 1998? The Gebran and Larry Page wrote a paper in the Stanford being graduated in Stanford, which was called Pedrank. That's was the beginning of the Google, and the idea was very, very, very simple, which we are going to discuss now. So we are going to discuss Pagerank algorithm starting from the flow formulation. And then we will go into the system implementation details or algorithm implementation details. Then we will go into the system design. OK. The main idea is that, hey, we need to assign some type of number of number denoting the importance of the page. To the page so we can write them. How we are going to do that? Should we consider incoming links? Should we consider outgoing links and? The idea was that, hey, let's look to the inlinks and as bolts and then assign some weights. Tweet and then. Using recursive algorithm which will recursive question we will. Assign more weight to the links that are coming from more important pages. Yeah, sure. Like it's an example of the PNC are much more important. So they have higher weight, while the others have lower weight. So each link, each link's weight or both, is proportional to the importance of its source page. The page J with importance. RJ has amount links. Each link gets a proportional width proportional number of the WOT. The weather by now. Then page JS on important the sum of the votes of its inlings. OK. So here you see that. J node J has three outgoing links, and. 4/2 are incoming links and each of them has some weight, so we sum that and we get the. Rank of RJ is. Ri third plus RK 4th. It is weight and then outgoing links have RJ equal to RJ divided by three. Rank so then it will be counted in the as an in the inbound. Links of the outgoing. Links and the recursive way. You can continue this for the whole life. This is simple, but there are lots of details that needs to be. So I want to see our outgoing things. Discussed, yes. Mm hmm. In and out. So there are, yeah. What's the question? For link. Yeah, I'm not understand. I don't understand what's the outgoing link. I understand if there is a link you can enter it. That's the import. So if there's a link on the page. Sure, go into the other pages. That is an ongoing link because you have like. For any page like I don't know, news, news, something. What it has links to some news and they those are outgoing links. Those are the arrows that are outgoing. Links from other pages. That goes to news. Those are incoming links. Thank you. OK. Can I ask question about this? Yes. Sure, go ahead. So you said this R means the rank of the node or the site, yeah. The rank yeah R is the rank of this node, which is the page. So everything in the web is a page. So in this case, if there are no any like in going arrows, it has no rank. Am I right? Oh, that's a very good question. Keep it. Yes, we are going to come to this back to this some point. But yes, there's no if there's a page who has no OK. incoming links, then it's not important in this algorithm, OK. OK? OK. So this is how you define it. The mathematical term is a sum of all all degree links from art to every J are summed by I divided by the degree of outgoing not N from node I. Pretty simple and this gives you this card, kind of what's called flow equation equation from operational research or optimization theory. You should be familiar with this flow thing. Do do not. Do you remember this? Well, did you guys? Cook. Operation research course. No. OK. Well, this is called flow equation. So we have a graph. We denote based on our definition. Each equation representing the rank and we get the system of equations which are called flow equation. Because can imagine that there's a number flowing from the one link, one edge to another through link. The interesting thing about this. Occursion is that this three? There are three unknowns. What? There are three equations, but there are no constants, which means there is no unit solution. There are set of solutions, so we need to add some type of constraint to this. To be able to solve it and. The obvious solution, the obvious constraint is to assume that the sum of all ranks is equal to 1. And in that in that case you will get a unique solution. Which which is. Which is representing the ranks based on our definition, and this is good. The problem is that. We cannot effectively compute this. Imagine if you have like billion nodes, right? This is this is not going to be a factor computed because. Elimination is not going to be fast, so we need some type of rewrite of this idea in the metrics form. So also we are kind of in this lecture, but showing that serious algorithms require some type of serious math. And if you remember linear algebra, then it should be quite easy for you. You are representing. Rank as a vector. For each page with the amount of. Probabilities going from this page to every other page and you have adjustments in metrics which shows some type of let's say probability or number associated with. Links going from 1 vector from site. I to to from page I to page G so. Imagine that you have 1000 pages in the web. You have 1000 by 1000 Matrix denoting that. What is the? Umm. Probability of going from. Page I to page J and it's denoted as a weighted some type of. Weighted sum or average sum of all outgoing links, which means that for each. Next step, you can represent your flow equation as metrics over the vector. Evan, so you if you have these weights of the whole graph and multiply it by the rank of the existing, you will get the rank for your for the for the edge that you are interested. To effectively solve this. Yeah, this is just an example, right? So. Suppose. Page I links to three pages including Gill, so this is our metrics app. Each has a weight of 1/3. You multiply it by the I and you get your. RJ. As an example, so we need not to solve this system factor. And do you remember? I get values and eigenvectors from linear algebra. Yeah. OK. So this is eigenvector. We need to find the eigenvector for the existing matrix. And to do that, what is the? Most effective algorithm if you remember the power iteration, you start with something and then you iterate over the. Initial vector and you will get your answer at some point because at some point the threshold of changes will be so low you can stop and here is an example of how to. Do that for a small graph of three nodes A1. We have the full equation. We create our matrix M which is just average which has numbers of, has an average of outgoing links. Right, this should be clear. And then you calculate rank as matrix over the previous one. OK. So now this is pretty. Obvious. Right. Or not. Yeah, which I think. Yeah, go ahead. If you have a question. Yeah, just a second. Yeah. No, I'm just. I just want to take a look at this little bit extensively to understand it. So you want to calculate the rank of your vector. 1:00 AM, right? Then what you do, you have your matrix or apply it to the existing one, to the recursive right. And you can get. Your. Solution. And it needs to be done in iterative model. So you need to initialize your right side with some numbers and then you iterate over and over again until you converge and the bulk converges will talk. So power toation method we initialized within. Equal number of and one and one and one and one else. And we iterate over until we was gets until the difference by modular is lower than some threshold that we define. And if we do this? But fast, we will very quickly converge to some solution. Yeah, modulo here is just. I want more. You can use anything. So this is one of the questions in the online research like how we are going to define the similarity. There are many ways to define similarity based on your target. You can define it using whatever you want. Heuristically, they found that is the best for us for this and OK. So we have some skills here. OK, it doesn't matter. So this is like the example of how to solve it. Just initialize with 1/3. You apply matrix M to vector 1/3. We get something and then you go over and over and at some point you will get. The last one and that's when you stop. OK. Umm. This is a math problem that if you continuously apply M and then you are going to approach the eigenvector for. M These are details that I just put didn't removed here actually. So you can take some look but this is out of the scope. But if you guys remember linear algebra, this should be fine and obviously so I'm going to. Only these details and. Go. Yeah. OK. Umm. OK. So this is. The basic idea of the Algor now there are going to. Talk about some of the. Properties on this hardware. What? How you are going? So there's a how can we update the weights on? How are going to update the rank? Should we keep it uniformly distributed or not uniformly distributed? What do you think? Hmm. What do you mean uniformly distributed? Umm. You are at some point in time at some point of time. Time T you are at the same page I right? So you have the vector of for. I which link which outgoing link from I you need to follow to update your ranks. Maybe all of them. Because like all of them are connected to Jay. Yes, that's one way to do it or the other. Hey, we. Assume that all outgoing links are uniformly distributed, so which is randomly pick. And we don't want to go forward to the all links because that is extensive computation and there's a proof that if you do, if you have a uniform distribution. Some after some number of iterations you are going to have the same result As for everything. This math problem. You can follow up this later. Well, but we are interested more is. Is there a solution right? And. I'm going to convert. And there's a theorem about this that if you because of the. Power algorithm power. Of power algorithm power. What was I forget, sorry. How are you todayation? Sorry. Iteration algorithm is represented. Can be represented as. Random walk and random Walk is a type of Markov process. Are you guys familiar with the Markov process? That the next state of the system determined by the current state of the system. Only and only by current state of the system. Are you familiar with that? Now we are. OK so. Then now that there's probability theory there is. A special field called Markov Process processes, which is. Researching those type of systems. And Pagerank is exactly the type of system. And there's a theorem proved that for this type of graphs, we will converge to the solution. Now OK, this is the math path and math part and you can look to the slides later or also you can look for the Chapter 5. The book managed data sets. Now we need to go from. Pure math construct. To multiple more algorithmic construct, because we need now to apply the math for this do something useful. So. Now this converge. Does this converge? To what we want. And other results reasonable. These are the main questions when we are looking to the power duration. Equation. So the does this converge is. Kind of looked into this. There are two problems related to the convergence and the fun is called dead End. What if we are getting to the one of your question to do? Node to the page that has no outgoing links. What are we going to do in that case? Right. What do you mean by what we're going to do? We're just going to ignore it, right? To ignore it. No, we all are. We get to this page. What are we going to do? That's the first question. The second question is across Pytorch trap. The Pyro trap is a set of as a cycle in the graph where you get into that and you just iterate and loop in in that construct and cannot get out of. The out of that cycle out of that trap, all all things are within this group. So to solve this we are using. An idea? Of teleport. So teleport is that if the with the probability B you follow link at random then with the probability 1 minus beta you jump to some other place. And one of the main contributions of Google folks that they found that this beta should be 00850.85. So at any time you have probability of jumping. Out the put this better. And if it's between zero, 8.85, eight and 0.9 some 5 cycles, you'll jump out. The same we're we're doing the same for. Didn't, but in this case we are jumping out with probability with beta equal to 1. With probability one, we're always jumping on. So this teleport notion helps us to solve the problem of cycles and not get stuck in the local network. OK. Yeah. So this is the equation written by bringpage 98. Where beta is equal to 0.85. OK. Now. Let's go to. So you'll look into this later. Your leisure time, how this is converging? But what's interesting is that OK, now we have an algorithm we. Explore some of. Its properties. Well, now we know that, OK. We know how to solve problems with the dead ends. We know how to solve the problems with. The spider traps. We know that it will converge. Because of. Representations of Markov process and. The corresponding math result. How do we actually compute rank? There's no system yet, so we started with the problem about ranking search results. We introduced mathematical notion. We represented a math notion. As a linear algebra and macro process. Export some of its properties and get to the point that we know that it is useful algorithm. It will converge. We can use it OK. That's the first part. Is the only the 1st part. How to compute it? Imagine that you have. Let's say 1 billion pages, right? Let's say you are. You need 4 bytes per page per entry. So you need 2 billion entries for vectors, which is approximately. 8 gigabytes. Now you need to have. N square entries for matrix and this is going to be huge. It will not fit into. What should we do? Ideas. I think Google has pretty big. Space pretty big. Say it again tomorrow. I'm I'm kind of. I think they have much space memory for storing all of that. Mm hmm. In one system. Do you know what's the limit for system memory in modern, let's say X86? Oh, I didn't know there was a limit. It's. How much is it? You have, OK. What do you think? Us you have taken system architecture course right? Yes. OK. What's the address? Bus with. In modern systems. Could you repeat? What's the what's effectively addressable memory in modern systems? You if it's 64 bits, theoretically you can address 2IN power of 64, but that's theoretically what's the effective. What's the effective limit? I'm just guessing it might be half of it. Not half, half is to the power of 32, which is 4 gigabytes, which is less than your laptop has. So depending on the system model, systems can vary, but it's starting from 48 bits to let's say 56. So that's the effective amount of memory you can address. But even if you have 56, it's in most of the cases of virtual memory, so. Effectively you can have, let's say, what 1234 terabytes of memory. 6 terabytes of memory, but not more. This and you have much more than 1 billion pages, right? You have hundreds of billion pages. So what to do? Yeah, this is about teleport, but more interesting is this. OK, we have. All of them. Based on this. Do we still need to calculate m * R? Yeah. What to do? First of all, we assume that. M is sparse. If you have 100 billion pages. There's no way all of the rows are filled. Why do you have sparse matrix? So. Average number of 10 links per node, approximately 10 times an interest. OK, does this help? With the computation you mean. Yeah. So what can we do? Differently. This is again about award. So here is the. Outward. I'm more interested about this. Yes, if it's parsed and you know the. And you keep only the meaningful notes then. You can effectively reduce the amount of space, right, Nick? And here's the just example. This 1 billion four bytes and you need only 40 gigabytes. To keep it. Of course it's much larger, but this is a good one. First step right? And then we need to do what is called OK. It's not here, but then there's other ways to do it. There's called sparse matrix encoding, special types of encoding of the vectors. Assuming it's sparse, which effectively compresses it, and now you can. Keep it on a disk. So you can gradually load it and do some stuff. And update it. Piece by piece. No, no. OK so. We start with the slice. It does it and. Won't work only on the meaningful parts and. Skip the. Skip the skip the zeros, but that's not enough. OK. Well, one of the things that we now have based on the size of matrix and size of vector, Now we can actually. Approximately cost. Now the cost perturation is. Proportional to the metric size and vector size or two times vector size because we have vectors on left or right. It's OK and good that if we can fit. R into the memory. What we should do if we cannot fit R into the memory? Rank is not fitting into the memory. What are we going to do? We do updates per block. We. Split the rank vector into the blocks and do block by block update. Clear with. Obvious, right? No, that obviously downstream. Say it again. Not really obvious, but it is understandable, yes. Yeah, now the. Complexity grows by the amount of the scans, because you if you split it into K, you need to repeat the iteration K times. What? To do if we know that M is much bigger than R, right? Can we do better? Yes, it's called strap block strap based algorithm. Each strap contains only destination nodes in the corresponding block, so you don't need to load everything, you just need to all the strap corresponding to the block. That's that's how you would reduce the amount of resources and amount of reads. That your algorithm needs to do. It's not a clever trick. And. Now we get into the next problem. OK, we said. OK. This is how we're going to effectively calculate the algorithm based on the block stripe method of the power. Of breaking down the power method algorith, how we are going to effectively completed implemented the computation. This is. Mom, any ideas? What are the concepts here? We already talked. Think about the concepts that we talked here. Besides the amount of Web page visited, it is important to have. The liability of it. Wait before reliability. There are central problems that we need to solve computing the algorithm, so the concepts here like we only talk it's Netflix and justice. The metrics on rank vector which can be a factor with computed using this block slap algorithm. So this blocks and straps are kind of consoles. We can map. Our algorithm to. But because if we go to the overall goal now from based on this effective algorithm that we constructed, we need to design a system where it to make system do something useful using this algorithm as its central. To make this algorithm run effectively and efficiently. We do lots of work, right? Because users want high throughput and low latency. So you need to serve billions of queries. Added 8 billion people. What do you think? Like how many billion queries per day Google is serving. Currently hundreds. Hmm. How do you calculate it? You can estimate how many Internet users are there in the world. 16 billion. 16. 06. Hi. You mean. Hi. You mean users OK 6 yes. OK. So we can assume that there's 8 billion people in the world. Let's assume 2 billion people doesn't have access to Internet. OK, reasonable 6 billion people in the world using Internet and how many searches are there? Also, don't forget that there could be some automated systems right? And doing some research, but let's assume it's we can assume it's 6 billion people. How many times a day you do such? 50 at least I think. Oh Lord. Say it again. 50 or level what was your number? A lot. Hello. OK, 50 is a good number. Yeah. So 50 * 6 three 150 billion queries per day. OK. So you need a system that will. It needs to have a throughput of three 350 billion queries per day. Also, you need low latency, right? You don't want to. Type query and then. Wait for a minute. You want that to come instantly. OK so. Somehow you need to make this run and make this very quick. And scalable. And you want to maintain low latency in high throughput as amount of documents grow. Correct. And you need you want high quality, you want to return high quality results relevant to the. Query. And. Ideally, it should be personalized. Of course we. Are discussing. Theoretical goals. Because the real system is much more complex, much more complex. So, but this is a good exercise to understand the real problems. That you should deal with if you decide to do such system. Also it should be fault tolerant, right? You cannot just allow it to. Fail and be out of search. Let's say one one hour, 2 hours, 3 hours. And it should give you up to date results. But if something happens in the world, you won't ideally instantly results be available in your system. Agree. Yes. OK, the. I'll just put this here to show you that how the naive implementation looks like looks like. Python And that the real implementation has nothing to do with this, because we're completely different. OK. Now we have some goals and requirements for the system. Now we start designing and laying out the concepts. What? Do you remember guys? The first 3-4 lectures that we talked about, the concepts mapping conceptual integrity, the composition, etcetera. OK. So the first thing like how to get pages. Right, so you need to start with some information, OK to construct the Patrick's Jason matrix and to have the rank you need to have the pages. How to get them? First question then how to pass them? If you pass them. What is the output structure? Usually we will use some type of index here, right? So corresponding to take this type of or, these words correspond to this page. This is index. Where I'm going to store the index and how to store it. What the iteration, what the power iteration looks like in this system, there are many, many, many other questions for tolerance et cetera, et cetera, et cetera. OK. So we should start some somewhere and just iterate over. To come up with some design considerations. First, I think that it's obvious that our system should have two components, 2 parts, one which will be offline pipeline which related to the getting pages, parsing them correctly, index calculating the page rank and. Preparing for the processing, the second one should be online. When you enter query there should be some type of subsystem that will gather query, process it. Translate it to some retrieval algorithm. To add the relevant results, rank them. Maybe use some of them else and then present it. OK. I'm waiting for you guys to nod. I was reading. That's written. But yeah, overall it's fine. The Ronnie. Yeah. OK. So the first part of the offline pipeline. Trawling this is a problem itself. It's a big problem, and of course we are. Everything we discuss here is hypothetical. Very very simplified because in reality the real system is much more complex, but it's good enough to understand the grasp of complexity so. You need some type of crawling process or also it's called web crawler. You need to start with some place which is called seed URL. So there is a core. Amount of size in the Internet. Created by Google. Which they consider highly relevant. And then that's the kind of the kernel, the place, the core where they start. For example, what do you think? Is definitely in the. The seed URL. The core URL. What is the? If you enter any term in Google, what is the first link that you get usually? I can enter now. Wikipedia. Exactly. Or. Good, yes. For example Wikipedia in the city URLs. Because it's considered highly relevant curated, trustworthy source of information. So. There are hundreds of others. I don't remember the exact name. I think it's about 1010 thousand or something like that. OK, then you have this process called spider or crawler. Which automatically fetches web pages from the Internet. Extract links all the hrefs from each page. At new discover links to the queue for future crawling. And then. You have the process which. Visits those URLs. OK, before using the dual you have this queue of URLs that needs to be visited, right? And this itself is a distributed system. So output of this system is the constantly updated queue of your routes that needs to be processed. Then you need to process them. Then you need to. Dub the page from the URL. Then you need to remove all the boiler plate, extract the main text, the content. Title hiters tags. Content there could be a language detection or language detection. Then you tokenize it, convert text into sequence of tokens, then could be words, phrases. This organization itself is a complex subway. Then you normalize it. For example, converting to lower, lower case, remove plurals, remove posters, etcetera. Then you do what's called feature extraction. Collect some type of additional signals. Anchor text metadata, keywords, entities names, people, places, et cetera. Based on site of dictionaries. None. Also, there's this Pam detection. This is the step when you do spam detection filter out pages. Based on some criteria or significant below their value in the output. Then you do link analysis. This is the place where you actually construct a link graph. Then do the scoring. The computer. The page rank is this second bullet only. You need to do page rank. On this graph. On store then you store the scores and to do this effectively one of the again return to Google. One of the methods to do this effectively on a large scale is called MapReduce. Have you heard about this? How good? Yes. Mm hmm. Yeah, so MapReduce, there was a system called Hadoop before that. Now there's a similar system called Open source system called Spark. Google uses some internal version. I think it's called big data big table. But the idea is similar. If you remember the strap block or block strap page rank each you store it in the separate node physical node physical computer. Some of the blocks you calculate this in a distributed manner and then you reduce the results. One of the nodes of many nodes and you propagate results to many other nodes to have a coherent and accessible. Turn off. So then you build what's called inverted index for each token. You store a list of documents where the token appears. Or IDs of these documents or some type of yeah, some type of items. This water leaks include some position information and also some type of metadata, usually some. Statistics statistics are related to that time frequency where it appears. Remember something else, especially for machine learning, it can be used. So this is the inverted index construction. And as I told, this process typically involves MapReduce. Then you store each document's metadata. Title, URL, language, et cetera. In two years distributed store. You sometimes or usually Google store some type of compressed version of raw text that you can't. You can quickly retrieve it. And. You optimize the indexes because this large indexes are often built in shards. Are you guys familiar with the sharding process? Not really. From database systems this is I didn't plan any homework for this lecture, but this is a homework for you. Explore what is Shard and sharding in the database systems. Why is it important? OK. Because this is going to be part of the distributed processing and distributed algorithms, we are going to study. It will be good too, so you know short. OK. Yeah, sure. And. Periodically. Not very rare, but not very often. You need to fetch new information, change change page and update everything. So all these deltas where it is needed to be update and scores needs to be refreshed. Depending on the importance depending on. Resource availability. It can be daily and weekly or even monthly. OK. So that was the. Hard related to the offline pipeline. Now let's talk about the online pipeline. Online pipeline is when you enter search and you get results. So definitely you need to have front end, right? You need to have user interface that. This is the query. And convert it to some API call. To the back end. Then you need to query parsing and interpretation, right? So you need to translate that query. To something useful that back end can process vulkanized normalized very similar to the document processing in. And then you need to distribute the query. Route to the query processes. Or query servers across multiple data centers because of the sharding, we talked a little bit previously. Here is answer to your. Here is an answer to your homework, but it will be good to. Read read more about shots so Shard is a vertical partition, Shard is a portion of the system available at some data centre and replica is the horizontal partition. It is used for redundancy. The copy of everything in multiple data centres. So. Based on the query servers process, you look into some of the shards, then you start. Searching in the embedded index for each query. For each token engine looks up for the corresponding lists. My document lists and you form. That's where you form the candidate set of documents. Contain the query, the query term. And then you do scoring on the ranking, ranking each chart. Calculate the preliminary relevance score for each candidate document using. The. In reality, there are other things included, but for our purposes, page rank is what we are looking to. Then you have partial ranked list. Because it's not a shorted list, you have many splitting indexes, so you have partial rank list. So you send all those partial rank list to the some type of central aggregator. Which does reduction or aggregation. So then you practically do. Merge sort and. Top. 105 fifty whatever number it is are consolidated. And. Uh. Oh, there's a. Yeah, there's a in real. There's a final word ranking because based on the list, there could be some type of other models involved. Machine learning, ranking model. There some type of personalization could be ads are being inserted in this part of the top of your search. And the list formed render. And sent to your to the corresponding process to which you are attached. OK. So if we look to the overall architecture, this is the very, very simplistic view of the fact so. We have crawlers, you have storage servers which stores in repository and indexer indexes, it shards them. Then you have URL which gives you the search. And tokenization splitting you send it to the page rank. And at the end of day. It's rendered and shot. OK. The overall flow looks like this. 2 interacting pipelines. Several distributed systems. But in general. Process wise crawler fetches pages, document parsing and cleaning index, building Link analysis, merged indexes. Then you have query parsing Shard lookups. Scoring aggregation. Presentation. OK. More or less clear. Yeah. So. We started from a problem formulation. This is was the idea of this lecture was we started with a problem how to do such and then. We come up with some algorithm. Discussed in math mathematical properties. Ensure that the algorithm is correct. Then turned into the computer science science part of that algorithm about its correctness about its convergence. And then we translated that algorithm into effective computation, and then we translated that effective computation to system. So. Path from a concept or a problem to the working system is very long and very. No problem. And the we wanted to show you, I want to show you the it's not obvious and requires lots of knowledge to get it done right. OK. Do you get you guys like, did you understand? At least 20% now and then. I hope that the rest later. 20 Yes, thank you. Mm hmm. OK. What other questions did you have now? Like what are the main question that you get is are still in your head and you bugging you? I will look up into shreds. I noticed it's vertical and it's interesting to me why it's vertical, not horizontal. What's the difference? Say it again. What? What's vertical? And the shreds now for the distributed systems. Bonds. Oh yeah. OK, rod is a yeah. Chart is a vertical slice. Replica is a horizontal slice. Or. OK. So far, because you have a very, very large think about the matrix. Now we go. It's a very, very large matrix. So you start with the portions of the matrix in the each computing unit. So you can do partial things and then you can aggregate. So if you can do it in parallel, you can work on short in parallel and aggregate. That's how you get speed. That's how you do it efficiently. OK. Thank you. The only question from you. Yeah, I wanted to ask, is this somehow related with just SE optimization? It's related with, right? Yeah, it's related. I left it out. There are many things left out like search engine optimization, machine learning, personalization, ads relevance. So there are lots of things which are left out because it will become very, very. Non obvious and non academically interesting. And so can we consider like all of this kind of basics of that optimization, yeah. But so if we go back to. Is. So search engine optimization is practically the metadata that you use that. No. So there's a metadata related to every document, right? I don't need put Europe as language and here is the CEO cops. So nobody knows how exactly it's being done. It's kind of this no how for each search engine, what is the amount? What is the metadata related to the CEO that is stored here and how that is translated into the index building? How is the documentary? Yeah. How it's translating here because. It has some value and some weight in these links. I don't know exactly how that's a question to search engine writers and. I think that partially it's being answered in this universal Waterloo course. I didn't get into that deeply. OK. Got it. Thanks. Any other questions? I don't have any. OK. Did you guys get the IDM the whole process how it works and how we get from problem statement to the working system? Yeah, yeah. And this process applies to any type. Of large or even medium scale programming, you need to be very clear what the problem is. What is the conceptual algorithm that is going to solve that? How the algorithm is going to help us? If it helps the algorithm correct and useful, how are going to present it and how this algorithm is going to be part of the system? Because if you look to. To our system that. You remember that page rank is a very small portion of the whole system, right? It's it's one of the central elements because it gives you the relevance, but it's not the only part. Everything else needs to be in place so page one can run. OK. Yes. Mm hmm. OK. And the last minute questions. Before I stop the recording.