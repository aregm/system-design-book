 OK. Today's lecture is about. We'll do short sweep and talk about how the design concepts are being applied in various. Applications. So in the first four lectures we covered the basics of the theoretical power part. Now we're going to learn how those concepts and the theory's being applied in practice. By studying. Actual systems. And designing actual systems. The I think that the first thing for system to start. Is unbeatable. We cannot do wrong with Unix. So we're going to start with Linux. This is a case study of how it's the composed and how obstructions are created to understand how the design philosophy is affecting. The composition and composition. Do you know anything about Unix system? Yes. OK, good. So then I don't know whether you know or not, but it's a very old system. It started in AT&amp;T Bell Labs. Bell Labs is very well known for being one of the advanced. That research. Facilities that many, many interesting systems came out and it started in 1969. The first version was written by Dennis Ritchie and Ken Thompson or Ken Thompson. Dennis Ritchie, in terms of contribution. It was written for PDP7 machine, which are mostly in assembly language and it was based on lessons learned from a very big audience. Failed operating system that was being designed by AT&amp;T called multiple soft. When the program was cancelled and the project was closed. This is part of the folklore, but I think it's true that he wanted to play Space Wars game with his colleagues and they didn't have any suitable operating system to run. So they start, they decided to write their own. The first version came out in 1972. Talk about that next slide and then in 1975 it was completely rewritten. Newly created high level language. Called C, It was created by Dennis Ritchie as a successor of a series of languages. The first one was a. Then it was. B. Then it was BCPL and the 3rd 4th one was C. And it was an unprecedented. What's your advancement for design? And implementation step before that time. Since before that. No, not on the operant system. Was was written in the high level language and after that. Most of operating systems were written on C. And. The Infosys. On the biggest lesson they learned from Malays, they wanted modularity and simplicity. And they put emphasis on this. This is kind of the philosophy of Linux. Oh sorry Linux building. Simple, short, clear, modular, extensible code that can easily maintain and repurpose by developers other than creators. This is a very. Loaded and very packed sentence and it means that it's very important, very important part is the last one that repurposed by developers other than creators. So if you follow some type of the development, then your system, your program can be reused by others. Do something good and here you can the right side. Here you can see the actually the four main influencers. There are many, but these guys are the four main influencers of the Unix, Kan Thompson, Denis Ritchie. They actually wrote the Unix system, Duke MC Kilroy, he wrote most of the service applications for. Or the first version of. Unix. Like add, word count etcetera, and Rob Park wrote. The rest of the application system. It is out it and created a book. Which? Is considered one of the classic books on programming. So it has a long history since 1969. As you can see it here. A birth most of the modern operating systems. Affected windows. Very big time. And the classification here that all the systems that are based on. Unix version. Four or five are considered Unix systems, and it includes. Polaris it in. It includes a seal line, seal, Unix HPA X. Most importantly Mac OS. And I think that you will be pleased to know that creator is our is Armenian, his call his name is Abedis Tevanian. And. It was the basis for the next step operating system, which then became Mac OSX. On the other hand, Linux. And Linux are not Unix. They are Unix leg systems because they are not descendant. They are not based on the source code of the Unix version 4. They rewrote everything totally on their own, so they are considered Unix like systems. So this is the short Xors to to the history. So we should move forward and if you remember. The recap of the previous lectures. Previous two lectures on the system decomposition composition. So we were talking that we need to breakdown system in order to under manageable parts based on the clear notion of the concepts to tackle the complexity. And conceptual integrity is a big thing that we need to keep on it, and we need to design deep modules with clear interfaces. This all is very reminiscent to the Unix because. They use the same way and we will see that actually all those concepts on design consoles came from Unix Peter as soon as you want me to. Could you let me know or do want me to ring further? Sure. OK. When was the? When was your first encounter with Unix and what was your? Impression. Not sure I remember the first one. OK, I remember my first one very clearly. It was Microsoft Xanax. It was Microsoft actually bought the system. The Unix system from. Labs and created multiuser at that time it was single user but. Proper Unix system for PCs. Then they dished it for in federal windows. But it was interesting experience. One thing I can tell it was very slow. And that's why nobody liked it. Because yeah, if you have proper security, proper separation of concerns, and Azure might remember that time the. The machine was 3/3/68. It was slow and. Any Mega House was the clock cycle, so everything was quite slow. That's why they dished it and moved to Windows. But it will be very interesting world if they didn't do that. Alek, what was your first encounter with Unix systems? We just had a curse call system. Last year and it was just about Linux comments operations and after. As well as well. We have a course named system programming. So we are working in Linux. OK. Very good. So origin of Unix philosophy came from Kent Hobson's early design choices. How to design small, concise, clean operating system with very capable service interface? And it's not a formal design method. That's why we're calling it philosophy. It's how do you approach the problem? And. As any folk traditions, it is bottom up, not top down. It is pragmatic and very, very grounded into the programmers experience. And there are several ways several. Manifestations of this philosophy. I think that the canonical one is the macroe 1. Four main principles make each program do one thing well. So if you want to do a new job build fresh rather than complicate old programs by adding new features. So this is clear. Expect the output of every program to be become to become input of another. Yet unknown program. Don't clutter output with extra information. Avoid stringently columnar or be a binary input. Formats don't insist on interactive input practically. It's saying that rely on textual clean textual. Implementation textual representation of the. Data that can be passed from one program to another and is parted by humans. Design and build software, even operating system, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts that will build them. This is, I don't know. I consider this first agile manifest. Way before all this trial thing came. To the stage. Because if you follow this doing small clean interfaces. Actual information. You can build things quickly. And it's one of the things, one of the manifesto, that it's better to have 90% program, 90% of the program. Are we doing good then? 100%. 100% of program not available which is true. Rely on tools. If you try to the 4th one, use tools in preference to unskilled help task. Even if you have to do 2 to build tools and expect to throw some of the somewhere domain it it says that to ease your job. Don't. Do. Clumsy and long, our work is better to spend time building a tool and then using this tool to do something good for you. And this philosophy is very, very. Spreaded in Unix. Things like. Arc things like. Laxer Lex and Jack, that all came from this philosophy, but it's bad. It will be good to write programs to write programs for you. So summary is that. Right programs to do one thing well. Write programs to work together. And write programs to handle stream because it's a universal interface for this program. In the Unix world. Rob Pike was usually referred as Commander Pike because this is a folklore. We'll refer him to as a commander park. Beyond that, and created this rules for problem 6 rules. 1st is you can't tell when the problem is going to spend its time. Bottleneck can occur in a very surprising places and unexpected places, so don't guess until you have proof where the bottleneck is. Measured don't tune for speed until you measured and you have clear indication when what part of your program is pending is wasting time. The rule #3 is very good it. Remind me the clothes, areggar clothes observation that fancy algorithms are slow when any small and N is usually small. Find the algorithms have big constants until you know that Ann is frequently going to be big. Don't get fanci. Even if N does get big, use rule two first so. When is the first time you. Were smashed by Ruth. I remember mine. I have a prominent example of this that I teach on the the other course, which is. Heap. Oh, OK. Mine is similar. Yes, go ahead. So this is a like a very nice data structure that's exemplifies. Better asymptotic complexity than any other heap. And yet the constants are so huge that tiny practical implementation fails to outperform. Even naive implementations at times. But mine is B3 example. When N was small and we I thought that that was 1993 maybe. Oh, it will be good to use B3 because oh, we are going to do indexing and things will be big, et cetera. But the card that. Number of list rarely exceeded 5500, so I was spending more time building. Structure and metadata than doing actual work. You have had. Yeah, you should have had like an array. Yes, exactly. That's how you learn hard, hard time, but good. Rule #4 actually are we do. Does any of this rule rules remind you something that you done wrong your practice? Actually doing the complexity class. We just actually, all my professors tell me that. Write shorter. Don't use these dead etcetera, etcetera. And during the complexity course we discussed. Complexity of several guys so. OK. Yeah, I think that the rule #5 is the most important thing. And this is you can hear this from the every experience problem. Data dominates. You desire a program around the right data structures and organize things. Well then, algorithms will almost always will be self-evident. And data structures. On central part of the programming. So you need to very clearly understand. That's why the stream thing is very important. Needs to work. Clearly understand where the data comes from, how it is being stored, how it is being transferred, transformed, where are you spending time, etcetera. And then things come together pretty nicely and there are no other rules. This is a book that I highly recommend everyone to read. Written by the ESR, Eric asked Raymond. He's a prominent. I'm in the Unix world. Our author of lots of good articles and one of The Pioneers of open source movement, and this book is. Actually, he didn't trust of gems. Have you heard about this book ever? No, no, thank you. OK. So it's quite old book for you guys. Mostly it's two. I think it's 2003. It was not republished since then, but it's you can find it. On net, and he summarized these 5060 no 40-50 years of Unix. Development into set of rules. And he came up with 15 rules. 15 was this 17 rules? Yeah, some rule of modularity. That you need to write simple. Write simple modules with clean interfaces and connect them together. To compose comparse things, clarity is always better than cleverness. Don't over complicate things. Is there problems to be connected to other problems? This is the very interesting rule of composition. It's called that. Descending from the Mcelroy's stream rule that you when you design module or a program, always think that something we should come in from other program and something should go out to the to other program too so. Who is? What is the what interface should look like? So it will be easily consumed. Wiler products role separation separate policy from mechanism. Separate interfaces from engine. This is important that I think it's called also modern optical and it will suppression of concerns that the product all interfere in the interface should be separated from the AMP implementations you can have. Clear interface of intent and you can have different ways of implementing that intent. Keep them separate design for simplicity. And app complexity. Where you cannot avoid it. Don't try big problems. It's better to have two communicating program small problems than one big problem. Design for visibility to make inspection debug easier. Yes, internally. Don't use anything. Not obvious. Use textual information. Use clear output so in in debug process you can see what's going on. Robustness comes from transparently in simplistic things are simple, simple and transparent. They tend to be robust. Full knowledge of data so program logic can be stupid and robust. This is. The rule #5 on rob packs that. Data dominates program logic. Data dominates program design. To start with data. Is there any? Reservations for the second part of the rules. Well, the one that suggests we build more programs instead of a single problem, a single program. May contradict the notion of deep modules, I think. Well, now that's interesting observation on how we're going to overcome it. I think what might be implied is that. If. This only makes sense if the interface for both new programs is small enough or comparable to the one large program. Otherwise you increase the overall complexity. And you elaborate on the example of, for example, open and read. No, that would be a bad example. I mean in that case this. OK. Open read. For open and read that does not make much sense to unite them. But I'm thinking about some kind of a processing tool. OK. I can give you an example that you might be correct, but still so lex and yak is a good example, right? So one is focused on. Mm hmm. Organizing Lexem is Alexa only the one. The second one is focused on matching the grammar. Every other tool in this realm is a single program other than Lex and Jack. Because usually you don't do Lexa without. Drama. But still I think. It has married. Because I think that the rule #13 is dominating your rule of economy program time is much more expensive. If you can write simple program. To do things well and then to combine two programs to do that thing and save, I don't know. In the amount of programs time, it's better than running one big program, but again, it's always a trade off, right? It's always a trade off. Yeah, I can point to another. Example of a problem with this like huge set of small programs that do one thing. So I had this in my practice when like for example you have to create some tool that does processing for some logs and you know that the. Linux. Tool suit. Should be enough to cover your needs, but the complexity of arranging the existing tools into something that works for you is. More. Complex than just building the one from scratch. Mm hmm. So this is the this is one of the problems that I see with this approach in general, because you have to know all of the tools that are out there to be able to apply them. Usually the the are applicable, but you have to. Go through the interfaces of each and one of them to make sure that for this particular problem that you have now you know what to do. Right. So yes, there's a catch. Otherwise, it's cheaper to just implement it. Here it's a Unix philosopher and Unix. I don't remember when it was written that Unix is not a user friendly operating system. It user helpful operating system. It still assumes the user is an advanced advanced programmer. Who knows what he wants to do? He can stay in a low level and that's and Unix is just helping you to do things faster. So it is the implication of the philosophy that Unix adding from. I'm saying it's good. I'm just saying that that one type of philosophy which is there. Is still dominant and you need to get. I think that it's always trade off. How do you define it? Yes, if you have because they're using the streams to get input output, right? If you have. A complex. Transformation pipeline. Which needs to be performed for example, and then you can always use single program to stay on the data and don't do streams. On other hand. One of the rules is that you don't know what will happen in the future. So in general, we have a kind of we agree that having deep modules is important. That overcomplicating things. Not overcomplicating things is important too. So that's where the I think that's where the art part comes in. Like, how do you balance it? And I don't have a recipe for that. I think it always comes from experience. I agree. And it's just like the. I think it's an example of the trade off between. More modular design and. Less modular, yet with a simpler interface. Yeah, and this is the example. I think it will be easier to for you to understand that doing one thing well is good example of cats, which does exactly one thing it conquered and the files and displays the output onto. The standard output. And that's that's the only thing it does. And the other thing is. True and false, and I don't know whether people know but aregiant commands in Unix and you can do. Them and combine with other stuff too. The true does nothing successfully and false does nothing. Period. So you can combine things and see what's happening. Those are actually useful when you accept or. Debug. Yeah. Or you accept a license when they ask for a lot of yeses. Oh yeah, right. Yes, we're talking about a composition that. In Unix, most operations have built through the write the standard output, and there are two operands, right. One is a pipeline pipe operand, which means that. Output of one program can be feed into the another problem and this CAD tax we see minus, L-L is maybe the most popular thing, I don't know. Are we this one thing that you guys did in the Linux? Yes, we did. No, so this is clear, right concatenates. Mar. Not concordance. What's your team name? Yes, on this you can do VC minus. Sign of smaller tax txt it will do the same thing. This is one of the design choices that. Unix. Very very deliberately. Made and this was one of the Kent Thompson brilliant ideas that he is considering everything as a fact, all devices. All the everything that can hold data and you can get data and write data into it is a file. And. This was revolutionary idea for its time. Umm. For example, device appearing for cat proxype info you will get your secure infoin exposed and you can do echo something into something which will get. Into. Alternate into the. The end of the file. Yes, simple program. Definitional simple program does one thing well and this one should be. Abstract or one thing that one thing is up to you to do fine does it. Well, should not have any versioning problems and can be used. As an input output for all the programs and here is an example of. Upper upper is getting. Stream or file? And. Converting it to the upper case. Is a very simple problem because one function this is. Go have you heard about the GO programming language? Yes, but. Great. Yes. No, but I've never tried it. It's very similar to see to some extent kind of we can think about this as an object oriented garbage collector. But it's pretty simple. You have one function which is converting to the upper case and then you have main which opens and just calls the function. So it does one thing, does it? Well, no. Versioning problems can be used. And this is another function which is splitting the input texts with based on the set of characters to split. And also does it well and now actually you can do pipeline, you can do cat uppercase split. Which is now you are doing a complex thing with three simple programs using pipes. And you can do multiple things. You can do uppercase please. Then you can do sort. Then you can in the sort sorted list you can call unique to do to give the unique list of things. So now you concatenated 2 texts. You. Ready to the upper case. Just put it in rewards. Sort them out and have it and call the unit. You have all the unique words in these two texts. Run through this pipeline. With the programs that does only one thing well. It's a good result. It required, yeah, the difficulties with such approach are that requires discipline. It's very hard to keep. This mindset. And requires lots of thinking and planning. Based on experience. And it's not science. It's not something that you can not a recipe. Nobody can give you the checklist to this 123 things and it will work. Sometimes it will work, sometimes it will not, sometimes. We're discussing with Peter. It's better to have complex than simple. Sometimes it's not. So if you know all this stuff already, then do you know, did you guys learn all this in the yours? Linux operating system course. Yes, I think. OK so. Just a quick recap. Unix. If you could remind me about Colonel. Say it again. If you could remind me about Karen. OK. So Unix is compressed and structured in a layered way. There is. Kernel which is the protected. Service program around the hardware which gives you the interfaces to the hardware and does practical. Two thing deals with the processes protects data and orchestrates the file input. Outputs orchestrates input output. Around the kernel there is a system call interface. All those things that for example F1 is a system call interface. It's the. Data fractions or set of libraries that you interact with the kernel to get things done on hardware. And around that, do you have the compilers, linkers, or what's called service? No. What's the right term here? Peter not service programs, but. System service programs or system programs assembles linkers, compilers. That. Gives you means and methods to write your own programs the next. Layer are shells. Shell is something that gives you access. Through the operating system to the machine, through a very simple interface. Practically what it does, it redirects input and output. That's practically what it does. With some commands that you can have built in and there are well known examples of shells. Of course, Bash is the most popular. There's. A shell, et cetera. And the other ring and the top layer are the application programs. If you simplify the Unix architecture, it's pretty simple. It's actually 2 levels, two level of software and one level of hardware. Hardware level is a lower level. Then you have the kernel level. And the kernel level. Abstract away the two big subsystems file subsystem which deals with the anything that you can have bytes stream going input in and out and processes which. Doing something useful in the inside the process control subsystem you have scheduler which schedules your programs to run. You have memory management, you need and you have inter process communication. They all are. Interfaced into the system called interface of the kernel level, which then goes into the user level. That's it. And that's why this is. That's why. This simplicity what makes Unix distinguished, and what makes it to be successful in this like 60 years of since it was created or not 5055 years. Since 1969. Uh. Linux file system, yes, is an abstraction. This is we're talking about abstraction and it gives you the uniform interface, treats everything as a file, and this is the. Default file system of Unix operating systems. It has the root. It treats files hierarchically. There's a root and everything else goes into the sub folders. Every file has. Execution mode and security mode. Initially there are three grouped in the three. Reset of two bytes so you can read, write and execut. And there are three security groups. Everyone, self and. Group. That's pretty late. The initial representation of the files were based on the notion of I node, which holds, which is actually directory hold permission file size. When the timestamps time stamps when it was created, accessed other metadata that can be used then there are pointers to the data blocks. Direct block and these direct block have. List of data blocks which where the actual file is stored based on. The types of the storage. Is it the spinning disk? Is it? SSD, et cetera. The structure could be a little bit different, but the overall design is based on the notion of I nodes. Direct blocks and data blocks. Yes, we talked about this. Process management and process management. Here also is kind of was done brilliantly for its time. 'Cause every process. Has. But let's go from the beginning. How did you define process like? How did you guys define process in the your operating system course? Question product. Do you mean like? Transgender relations or just? The practical process is. I don't process. Is anything that is being can be executed as soon as you start the IP IP register, so you have. Code segment defined your data segment and your instruction pointer. Set and start executing your process and first time the Unix would correctly and kind of differently from anyone else. They started with a single process. When you boot, you only have a process. Now the question is how well are going to spawn all the other processes in the operating system they came up with a very simple and brilliant idea that there is a system call which is which which is called fork which forks the process into two parts. Parent and child. And you get different. Return code in the child. In the parent process and in the child process, you can call exact function which is executing different executable and thus you are the same way as you created file system hierarchical from the root. You are creating processes starting from the zero or the boot process. You're *******. I'm not looking. This was brilliant and it was very simple, very manageable, clean interface for a very very, very long time. What? What problems do you see here we have here? What problems do you see here that? Can arise. Let's let's put it that way. Peter. OK. Go ahead. No, no. Is there any any? There he is. Well, just looking at the. Scheme it might like suggest that what if forking gets expensive? What if like copying my own process, is my own process is very large for whatever reason it has like. Grown into many parameters, many settings and now forking becomes expensive. Yes, that was a very good thing. A good observation that it was really a problem. Then it was optimized through other lots of means. How to do this effectively? The other problem which? Arrived in 2000 is multi threading. This model does not. Naturally support multithreading. Yes, we can say it's something inside the process, but what is the relation between the process and thread? How am I going to capture this correctly and this interface? And it occurred at this interface, you can't. So that's why the move to the POSIX interface and other P threads, which are kind of orthogonal, process all of this. Anyway going forward. Peter, wanna take this about pops and Redirections? I think you've covered most of it. Yeah. So practically, what do I say? We're talking here that there is a kernel or call operating system based mechanism to redirect. Outputs standard outputs and that input and standard error. Between programs and. It's the tool how to compose. From the simple parts practically. It's a choice between message passing and shared memory. Message passing is a philosophy that says. You communicate through the message passing message, shared memory, shared memory. I don't know whether you discussed this in your course, but in essence. Are we treating the communicating parts as a single process or multiprocess? And how do we share memory? Because as soon as we start sharing memory between multiple actors processes in the operating system, you are getting into the problem of. Data consistency. Data races because you need to keep the shared memory. In the correct state, right? How I'm going to if two if two processes are concurrently accessing, how I'm going to do? It. And we read after write can write after read and all those things which. In practice, system theory, we are calling memory model not problem languages two, so that all those things arise. And it's not easy to do it correctly. On the other hand, message passing is pretty simple. You communicate through message pass. Doesn't matter if you want to something to be shared past message and this is a philosophy that is very. Unit centric, for example. And the tweet will then. Generalized in concurrency theory via. Formal methods, for example communicating sequential process algebra created by 24, which is a basis of concurrency in probable language like GO came from this. Yeah. So this is that single responsibility principle. Each program does one job and does it well. The example commands. List grab sort, awkward count, et cetera. Filters. Yeah, filters are just. Type of command, type of the program that just filters out something from the stream and. If you could fill the string back to the same input or up string output string so it is useful to when you combine in the pipeline complex. Yes. And I think we all recovered this. That there everything is a file. There are two types of devices that are represented as the files either tractor or block devices. And. You can treat them as a file. You can read them using the right access modes. I think this is an interesting case of. Enabling functionality at the edge of. Abstractions like this is what we've talked about in the when we talked about Volatilitybased decomposition. So here we see this. I think in action. OK. Like for brand device random right? This is like has nothing to do with files, right? But we are implementing something through the obstruction that was not kind of. Built on top of this particular area or the main functionality. So it's and yet it provides a nice way of obstructing that. Yellow bright on this. So. So you are be kind of be there was advocate here for example dev random run but I can argue that why not to do so dev random I can do import math. Random call random function. Sure. It's just about the composition. So the composition becomes much simpler, since now you have the bot mechanism and now you can just plug it into your pipeline and use it as you would use any other. Yes. Something that is not random. Ari, did you get it? So what was the random maternal again? It just returns random stampling. The file you can read from the file and will return a random number. OK. And Devnell is a kind of what is called termination. You can redirect any stream here. It will do nothing if you want to terminate the stream, you Huh. redirect it down now. Change. So what Peter is trying? Alberting you what you, Peter said, right, so that. This if you look to the our concept of world healthy based design, it occurred this special. Files can be very useful if you want to expand your functionality. Without changing your interface. OK. OK error codes is another thing which is very very very. Unix. Centric I will set. If you do something in Unix command and it's successful then. Everything is silent and this this comes from the terminals. The first terminals that the guys were programming. Were not. This plastic well, typewriters and if something does happening, if something is successful, you don't need to do anything. And that's why Unix if everything is good, no response. But if everything everything is bad, then the philosophy of Unix says. Shout and scream. So first of all you need to return the error code. For the operation that you did so 0 is. Usually everything is good and then you have error codes that are propagated through your system. Till the very top. The one. From design perspective, one hand it. 'S unified. And easy. But what's the drawback of this? What's the downside of all this? Do you think Barack? Those who program and go know this very well. I don't know. Problem is that every you need to check the output. The return for every function and this is called. I don't know error hell in gold, whatever they call it that you call a function, you need to check the output. And because in goal, for example and Unix, every function returns something, it returns the map whether it's successful or not, and then if it's not successful you. Need to immediately halt and propagate that that. Are to the call it to the, to the caller. So. Modern programming languages are dealing with this with the exception. But you do. You have the natural form of your code, and if something goes wrong it throws an exception and then you catch the exception at the bottom of the code and you deal with the problems there. Two different philosophies, two different ways. Both of them are OK. Both of them are good. Depends on what are you trying to do. Peter what? Which one do you prefer? Well, I definitely hate writing if. Error code didn't go. I I hated it so much, but on the other hand, I hate to try to understand where to catch the exception so. Yes. Both are terrible in my opinion. OK. What's the idea solution for you? I don't know. There is no good solution. Yes, unfortunately there I don't either. I think that. Goal is some extent. Something good, because they say, hey, return to topple error and output. Yes, you need to do this even else if then else if than else. The only on the other hand, yes in C++. Exception can be raised and you don't know where it came from. In court, and do you? Do we need to check all the cases or just is it 0? Is it? Is it OK or not? I'm so, yes, I'm gonna go back to you. Because because. Go call them. OK, give them. Phone Callan. So here I it's not. There's a. If my friends too, I can't. No, this is different, but here upper function right? You call upper function. It actually needs to. Return yes here. Open. If error then you say if error is not nil then do something else. For example, write to error. That this function is not is not cannot be done. Mm hmm. And this is the case for every function call. Every function call in system or in your program should return a tuple, something and error, and then you need to check if error is not equal to now. It's very exhausting. But completely explicit. And you exactly know what is happening where. I think they did something, Peter in this latest version. Helper function. I don't know what they did, but do you? Do you remember what they did? I didn't touch go for several years, so I don't know. Yes. It worked here, right? So yes, we're getting to the end, which is good. So what Unix did right? And there show is an abstraction layer. So Shell is the explicit command interface to do to interact with the kernel and to get needed functionality from the hardware. Down through the commands and it gets the command translates into the system calls. Chain of system calls and returns output to you. Yeah. For example the command list. Is not a simple command. If you again, this is actually. This is a good example of a deep module because it does lots of things. It's checks where are you? What permissions do you have? What is the current path? And then goes checks all those permissions, goes checks all the execution mode, then gets the information from the file system, gathers all the. Information into the buffer then finds out in which mode you are running the LS, how the op buffer needs to be formatted, and gives gives it back to you. So it's not trivial command. And now Shell is in. Otherwise you will put it every command line is the preferred way of dealing with. Interacting with the operating system for some operating systems and for some problems. Not everyone, so. Principles and for that we can learn from eunux philosophy philosophy for our course. Small, modular, composable. Are very important. Try to do that way. Small can be treated multiple ways. Either it's small. I would prefer to treat small in terms of the interface interfaces, but not in terms of the size of modules. We have the for example the F open case. It's a small module, but it's deep module, so I think that will be correct to say small deep modules. Umm. Compuls through this Clean module, crew clean interfaces. Into the much bigger modules. And that the composition of the small interfaces is the preferred way of dealing with complexity. Yeah, so this is what we are just talked. And are we? Yeah, this is homework. Which you guys most likely will ignore. So you have a question about your homework. Go ahead. Not anymore, 'cause. Not anymore. OK. OK. So that's all for today. So next. If you wrote, write your spreadsheet program, we can analyze and give you feedback next time. But next time we're going to discuss kind of tactics of more about tactics of the design and the way you will start with some build systems. I'm going to write a build system in Python. And we are going to learn how to apply all these things in practice. With much more emphasis on. The Applica programming or the write code? How to command it? How to put them like into test et cetera, et cetera? OK. Any other questions? Say it again. No, no, I have. I don't have. OK. And that's it for today. Stop recording.