 And he's going to talk. About relational model and design of a modern database systems. And there are very very few. Scientific papers that. Change the field. Especially the computer science field and one of them is definitely codes. Uh. Papers, actually, three papers. Where he defined the relational model. Defined relational algebra, defined relational calculus, and showed how algebra can be a mean to calculate the calculus and reason about the calculus and what are the implication for the. Data processing in general and what are the implication of for data database systems? And today Peter is going to go over that part, but less less focus on. Theory and we're more focused on how actually. We're going from algebra to a calculation or computer. OK, Peter, stage is yours. OK. So the plan is simple. We'll go through the early systems. What was before the creation of Religional relational algebra? What were the problems that the revolution algebra tried to solve? Then we'll look into what the relational algebra is, and especially what the extended relational algebra is. How does it map to? Modern database concepts, the language, and then we'll look into how the modern database management systems are built. What components they have and how do they solve? How do they implement the obstructions to satisfy the? High level promise of modern database system. And feel free to interrupt me as I go. I don't mind at all. So first the sources. This is the book. I think it's great if you wanna dig into databases. It contains everything you know need to know to understand how. The A databases system is built, at least on the basic level, although it does have some advanced stuff as well. And there is. Probably the best course. Ever. On databases from CMU. Hosted by. Highly, highly recommended. I think the link goes to the basic course, but there is also an advanced one which is somewhat more interesting in touches on the things that I'll briefly describe today. In very great details. OK. Let's start with the definitions. So what is EDT BS? A database is a collection of some data that models some aspect of real life. Databases are everywhere of course, so this is the core part of any modern application. Ranging from. Telecommunication services to I don't what everything. We call a database management system DBMS. A collection of. Data itself and a set of utilities tools, programs that allow you to access, modify. Validate, clean, etcetera. Everything you want to do with data. Usually when the. Database database is built or designed or implemented. It requires something called a data model. Which? Is basically a set of concepts that describe how the data is arranged at a logical level. It's very easy to understand what that is. All examples so here are a number of existing data models. The list is not exhaustive. I think it's it goes on and on, but there are probably the most important one. The first two are the early days of database systems, and we'll talk a bit more about them later. The hierarchical and the network one. Then there is the entity relationship which is quite useful even today when designing a database. And I'm saying a database, not a database management system. I'll be I'll try to be precise with the language I use. Then there is object stores that allow you to. That there are many systems that deal with arbitrary. Objects like storing Jason files. Dean, there are very popular key value stores. Some use graphs, some don't. Then there is of course the relational model. That most of modern database systems are built on. And there is. I've included tensor as it relates to AI and stuff like that. So let's look at what. Database system. Requirements are and. To do that, let's look at the very simple example. So here on the right. I've created my first crude database. So it was very easy. I've created a file. A CSV file. I've created a header with the names of the column and I put some data in. Then at the bottom I wrote some query in. In pseudo language, right? So. This query opens the file and tries to find. The list of names of students who attend. Certain course. Now, if you remember some of the lectures, some of the first lectures we've been talking about, the design spaces and requirements and how the obstructions are born through the points of friction. So to identify those, we usually try to. Foresee some of the changes that might occur. So this is a small exercise that I've done here. I've listed some. The possible changes, right? So. For the for my solution for the problems of my solution to to become apparent, right. So what happens if I start accessing this my database with multiple users? Well, if they only read the data, then there's nothing's gonna happen. But they if they start modifying it, I'm gonna break something. What if I change the type of the fields and I didn't account for that in other records, so I can break what is called a schema? And you can see other many more examples that I'll list here. If these questions lead to the primary. Primary. Goals of a database system, right? So it provides consistent access to the data that allows multi user access with guarantees on data persistency and it performs tasks such as validation on input permissions and so on. The list goes on. Obviously this is not a new task, right? So. People have done databases for hundreds and hundreds of years. It's just the way we build them changes over time. We are, of course, interested in the most recent developments, but let's start with with the early days of database systems. So the database systems we play. Prior relation in algebra take roots in 50s and 60s, so with the. With the. Transition from sequential data access to random data access. This is the time when the hard drives were created. And the data processing. Benefited significantly, right? So the first database is run on mainframes, which is basically like a huge box that you. Load all your compute on. Which had quite a high operating cost. So the humans were cheap. The machines were expensive. And people tried to use those machines to manage the data. So like if you would work at NASA in 60s or 50s, you would. Store the data about all the components of the whatever space program you take in some kind of a database system. Which is probably would be an IMS. The programs that were used to store and capture that data. Featured either network or hierarchical data models, which were basically. Consistent of two types of objects. Links and records and so you would well depending on the model you would do different things, but you can imagine that properly hierarchical is an easier one to imagine. If you have like a set of. Courses. Then you would you. Maybe you want to create. A new. Record that describes a student and then make the course apparent of that record student. So then you have a link between the two and you can follow the data through. Just as in my example. In the previous slide, right. So these the programs that users had to write. Heavily depended on the structure, the underlying structure of the data, right? So in order to query the data, and especially if you wanted to query it efficiently, you had to know exactly how does the data look like. Looks like in on the hard drive. So the bit sequences must have been known to you. This is pretty inconvenient, but the models survived. I think until 80s or something. Just because the relational algebra did not bring the. Benefit immediately. So at this time, we've got these three major trends trends, right. So the there is like the data volume growth. You can say about you can say that about any period of computer science. But anyway. That's that was the trend then. There was very high meeting mainframe operating cost and there there were disks hard drives. I'm sorry, could you? Yeah, this. Back yes. Sequentiality here. Does it mean that we need to go through all the records until to get the one we Mm hmm. need? So I've put this. This relates to the physical way of accessing the disks. So before the. Hard Drive was created. You would actually. What you would store the data on magnetic tapes and magnetic tapes? Were the the way of accessing the data on the magnetic tape was sequential, you couldn't like jump between the two random pointers in memory. So in the 60s, when the disc was created, you had now an opportunity to get rid of that. So you could like create a table and access a specific record. Somewhere in in the middle, if you have the correct pointer. Thank you. So the programs had this in the database management systems had tight coupling with the underlying physical layout because of the data model. So clearly that has many disadvantages, right? So here I just named a few. But you can definitely think of more. This was noticed well, he had to work on that as well by Edgar Cod. So he was working as a mathematician at IBM back in the days in 60s. And he was constantly looking at people, rewriting over and over the algorithms to access the data. And he. Decided to try and work that around. He came up with an idea. That he named relational algebra. Here is the. Original paper that is called a relational model of data for let's share databanks. The idea was three fold, right? So, or you could say. All he wanted is to get rid of. The physical knowledge, physical layout, knowledge as a requirement for writing queries. So the idea was to develop some software architecture that would allow you to not know anything about the physical organization of your system and still be able to query the data. To achieve that, he proposed to store data in a very simple data structure. He called this relations, which are basically tables in layman terms. In the database, axis is performed only at a logical level and you do that by using a high level language, so it doesn't. It wasn't sickle immediately, so there was just a notion of a high level language that was rooted in the relational algebra. All the details. About how the database management system. Performs and access and queries and maybe changes. The data was left to the system. And this was this was a nice idea and. The he tried to promote that at IBM. And funny enough, it did not catch fire immediately. People were still unsure whether or not this was enough if. Database management system could optimize queries good enough better than people. Basically the same story. Has happened with compilers in early days of compilers, right? People thought that. Humans can optimize code much, much better than the compiler, which is clearly not true today. So this was a repeat of the same story. And so they, together with other researchers. I'm. Of those. I guess a prominent name would be soundrecker. They developed a system called system our project. That was. It wasn't exactly a relational database system, but it resembled 1. And it had a high had a high level language which was close to SQL. And IBM later on, actually not before. Competition, which was Oracle of course. They started developing an actual production grade solution. I've just listed two like SQL, DS and DB2. I think DB2 is still around. It will for quite some time. But these are all rooted in God's work. All right, So what is relational logic? First of all, it's an algebra, right? So it exhibits all the mathematical properties of an algebra. Not exactly. So we talk about extended relational algebra because the. The. Definitions of the operator of this algebra are not exactly clean intermed in math terms. But for our discussion, it doesn't really matter too much. So relational algebra relies on set theory and adds some more restrictions and some more random operators that. That are used for by database management systems to perform queries. So each operator transforms one or more relations, right? I I use relations and tables interchangeably, but of course the correct term is a relation. A relation is just a set of tuples and the tuples is a member of data domain. Data domain is a bit awkward term to describe. We can just talk about them as attributes, which is very natural to do right, so each. You may imagine table and has multiple columns. Each column is basically an attribute. So here you can see the basic operations. Which is a selection, right? And I've put the. Standard symbols that specify the operation. So Sigma. Selection. Just selects some tuples from the relation and creates a relation out of this selection, right? Same with projection and so on. Also you can see that there are like. At least three. Classical satiria operators. But there are more, right? So we usually add a join. Which can be. Represented as the basic operations, but it's so ubiquitous, so we put it as a separate operator. There is division of course, but it's not usually included as a primitive operator. There is renaming that allows you just to manipulate the. Names of the relations and there are some auxiliary operators which are not exactly fitting into the algebra. This is why it's called extended right so. An aggregation produces a single output, which you can argue is a relation, but it's kind of not. Clean. Sorting is also like that, right? So set theory, your tuples are unsorted and databases usually represent them in unsorted way, but provide sorting primitives. So when you take these set of operators, you can chain them together of course, because each and every of them produces. A new relation. So you can compile complex queries. Let's look at an example. So here I've developed. Come up, we go further. M. I think it's very important to. For historical reasons, to understand that this relational algebra and subsequent relational calculus didn't come up directly from the paper. So the 1st paper actually there are three papers, right? So the relational algebra term appeared only in the 3rd paper, which is 1972, which has all the things strictly mathematically defined. It took three years to refine. Algebra to be mathematically sound and to express how. Alkylus will be implemented and how this is going to be? Complete. Because it's very important to prove the completeness right, because otherwise you can how you are going to build something solid if this is Mm hmm. not solid. If this is not complete. So all that happened in the 3rd paper that caught produced in 1972. With slightly different operators than here, but practically the same thing. So the idea that Peter and we are trying to express here. Is that how you go from the farm? Similar to the previous lecture, but even more on a higher level? How you go from the higher very high level concept which is very sound mathematical concept, not even algorithm to algorithm to an algebra to a calculus of how do you express that in a particular database management system that can compute. Queries. OK, is this is this clear? For you guys. Yeah. Any questions here before we move on? OK. Let's continue. Thank you. Mm hmm. OK. I'm not really familiar with the term aggregation. Aggregation is. Let's let's take it by an example. So you, let's say you have a column, right? Of ages of students right in my previous example. Mm hmm. An aggregation which calculates an average age would be an aggregation, right? So you just. You aggregate many values to a single value. That's an aggregation. Mm hmm. And the reason it's. It is like. A separate operator is that it exhibits different properties on, especially when you think about parallelization, because you cannot. Well, there is a certain degree of parallelization you can achieve, but it's not infinite, right? I think it will become a bit more clear when we talk about parallelism. Mm hmm. And there's a fundamental thing. It's that here we are on going. 2 fundamental transitions. One is. We're not introducing, but there is a relational calculus which by reduction algorithm goes into do algebra. So algebra can serve as a vehicle for implementing calculus. You should remember this from the mathematical logic course. And here the same. No. I think cod does. He's he's saying that given a calculus based language like sequel or that type of post, it just was called Quill. You should be able to take the query submitted by the user. Just basically calculus expression and apply the reduction algorithm to it and then to. Thereby you are going to obtain equivalent algebraic expression. That is, it can be computed effectively, and for that you need to say that the language this algebraic language algebra needs to be relationally complete. Powerful as the relational calculus, so you are not going to lose anything during the translation process, which means that in the relational model means that any relation definable by some expression of the calculus of the relational calculus is also definable by some expression of the language L repres. Algebra. OK. This is theoretical. This is as much as you need for the theory, and let's move to implementation. Sorry Peter, I can't process. Any questions to that? Yeah. Yeah. No, no, that's OK. Thanks. OK. I don't hear any questions, so I'll move on. All right, so here's an example, right? So here I've built a very simple database database, right? So taking my first example to A to the next level, I have 3 tables, SO33 relations, right? I identify them by their names. So there is a student enrollment and courses relations. And here I have a similar query. Well, maybe it's the same query, right? Yeah, it's the same query that. Gives me an answer to my original questions right. So which students are enrolled in the in some course? Here I have the left to right right. I'm reading the relational algebra expression left to right, so I have a projection of of a name, right? From a relation that I filter. By a course name and inside I'm joining the three. Relations that I have on a certain field right on I'm joining these students and enrollments over student ID and I'm joining that on course ID with courses. When I've done that, I'll just end up with the with the same answer I was getting from my. Program that I wrote to go over the fields in a CSV file and. Process the all the physical details of the data representation right. So here I don't have that. I only have the relational algebra expressions and everything else is hidden from you. Is it clear, guys, just before we move on, like, do you, do you understand? How the query is represented by algebraic operations a superposition of an algebraic operations? Yes, no questions. Levon, this is important point. Yeah, yeah, yeah. Mm hmm. Yeah, I got it. OK. OK, great. Perfect. So now now we try to move from the algebra to the language, right? So. For. Relational database systems to be. Easily usable. They need to provide some interface, and that interface is a language. So we want the user to describe the what what they want, not how they want it. So the language should be. Rooted in the relational algebra principles, and of course there are deviations. Because of this is a somewhat programming language. It's not meant to be Turing complete, right? So it's only a query language. So I'm putting the SQL of course here the structured query language has de facto the standard. It of course there before, like in the early days, this was not the only option. The input I'm saying this is original developed at IBM. Right and called SQL. Well, actually it was first called QUEL and then it become became structured which was born from the system R project. But later it was picked up by phone breaker and one to build Ingress. Ingress is a prototype for, well, not the prototype, as it was actually. A. Fully fledged system but this was what came before. Or Postgres and Postgres you may know exists today. The language had. And still has a lot of dialects. So each vendor and each database management system implements it differently slightly. It can follow or it may not follow the standards to do the letter. Yet there are the there are standards. Probably the most important one is the sequel 92, but there are committees that. Are performing and publishing news standards even today? So here I'm rewriting the relational algebra expression with the SQL language. I'm using the like this this 92 standard, but I think you get the idea. Is there anything here? That. More explanation. Alright, this is clear. The query is pretty straightforward, right? No questions. Perfect. All right. So when we look at this language, I'm taking SQL as the main language because it's the defect of standard. It actually consists of multiple sub languages. So. There are I guess there are different classifications but. This one I think is simple enough. First, you have the data definition language. This this is used to create objects within the database like creating a table. Look at the right and creating a table of course, and I'm specifying its schema. I can create users and other objects inside the database. Then there is the data manipulation language. This is the core of the language. This is what basically. Any queries would use. It is used for both reading the data and modifying it. And finally, there is the data control language. This is used for accessing the data in terms of the permissions like I'm specifying the policies I assign users to certain permission groups, et cetera, et cetera. Of course, that's not all right. So modern database systems provide more things. There are transactions, right? That's an important part and I've put here the TCL and I think it's called transaction control language or something. That's it's vendor specific. It doesn't really matter. There will be something that is. Related to transactions, so you make sure your changes are quote UN quote committed. To the database. There are also things for constraints and integrity control, and there there is certain layer of obstructions called views, right? So it's very simple as you understand, you have a huge database with all the tables. Related to your say enterprise, but certain user groups need to work only with a portion of that database. So to obstruct that away, you may create a view of that database. So of course there are language level tools to work with that as well. Now we can look at the. How the database management systems implement? The promise of performance accesses to the database without knowing how the actual. Physical. Data layout is built how the access is performed, et cetera. We start with storage. So if we look at back at the early days of relational database systems. And look at the systems that are working with disks. You may find that not much has changed in terms of the concepts used. We still store the data as blocks on disks, right? So the there are we of course we have multiple disks and they have. Random access. But in the end to in order to access some portion of the data on the disk, you have to create some kind of a request. To the driver and it would load a block of data to the main memory. There are certain properties associated with that process, right? So the there are certain use cases of this scenarios that are optimized for in the OS and in the hardware. Of course the yes. Here I list some of the parameters. Of accessing different data locations in the memory hierarchy. This is pretty old, but the take away here is that. As you go closer and closer to the processing unit, of course the data access data latency access gets smaller, but the size also gets smaller, right? We would love to store all of that, all of our data in L1 cache of course, but the size doesn't permit it. So we have to retreat to other. Layers of data. Of memory hierarch. With that said, of course you want to minimize the long outstanding. Queries to either disk or network. This is especially relevant for things like clusters or distributed store. So the optimizations are quite the same. Now imagine a very simple scenario where you just looking for the data, right? So you are. Say you are entering a website and you hit like a search bar and you type something to search for some product. In order to implement that, one may just go over all of the records in the database, right? Fetching everything from the disk and check if that the thing you've typed matches the description of the product in the database. Of course, that's not what you would like. Do you want to minimize the number of disk accesses if possible? If you look a bit inside, right? So when you talk about blocks, that usually means that you store some chunks of data in your relation in your relations inside those blocks. My example here features a very simple scenario when you have just two records inside a single block, right? It may be any way you want it right? So the there is an obstruction that the database management system provides internally that abstracts that away, right? So in order to access the data there, there are mechanisms that know exactly how the data is compressed, how the bits are laid out inside a single block. There is usually a header. That tells you what offsets to use to access which fields, which column to access which. Which record within the block, and so on. There are like whether it's it is null or not null and so on. So if I'm going straight from Frost's record to the next record, there is only one fetch from the disk that is required. Now if I wanna find the one of the. Let's say some kind of record and I know its property. I can build indexes. Indexes may allow me to find the data faster and by just minimizing the number of disk accesses. So this is usually a tree and I actually have an example of that. I'll show it bit later. So you just use the your basic usual algorithms to convert the linear scan to logarithmic scan, right? Because these this could be a balanced tree. So you can lower the number of disk accesses to log N. Index itself is of course also can be quite large and there are usually multiple indexes, so index would be a separate record in a way that is also stored on the disk. So in order to fetch some data, you first fetch the index. You walk through the index, you find the block ID or the offset of the block that you're looking for. And then you access that block. Of course, since if you have more and more data. You your index grows right. So it becomes not a single block on the disk, but multiple. Here of course you wanna build up and have multiple indexes into indexes. So you have a complete tree. Any questions about this? Yeah, I have a question about like a charge here, I think. I don't know. You spoke about or not the the size of the chunk. How are we? Like how we are defining this one. Is there like some kind of size for that or not? This is defined. Well, this will depend on your hardware architecture and depend on your way of storing the data. Usually discs have, like the physical disks of the devices, they have certain parameters that tell you. Which size of a block can be accessed at a time right? And there are certain OS APIs that allow you to do that. When you are building a database management system, you are aware of that, right? So you would optimize your layouts to fit into a single block so your chunk size that you want to put in in a single block needs to be. So. Packed and so maybe compressed and so on. To make sure that you are not overlapping. Between two blocks, because if that happens, you would need two accesses to the disk instead of one, which is like twice as slow, right? If your record gets big, that just means that you either have less records inside a single block, or you store them separately somehow, right? So you can have slices of records stored in different blocks, which is usually the case in modern database systems, and will mention that a bit later. You can have like column or row store. Here I'm talking about like in very simple terms. I'm saying that each like. Row in a table right is stored like. The rows are stored sequentially. That cannot be the case, right? So you could store them column wise, meaning that if I look at my memory, the linear offsets they consist of a single column not not the row. Does that make sense? Does that answer the question? Yeah, yeah, I got it. Thanks. Here's the just so you know the the primary. Data structure for storing indexes is called AB +3. It's a self balanced tree. And it's also called Amway search tree. It's like your usual binary tree, but it's not binary. It's still a search tree and it's perfectly balanced. So it also exhibits some properties that are useful for databases, such as. For example you can. Store the data in the Leafs and have no copies of of that data inside the body of the tree. Now. Now we have like a very oh, I would say very shallow understanding of the storage. But I think that's enough for. For the purpose. The I guess the take away is that we somehow created an obstruction of the. Of the way we access the data. So all of the details of how we do that are hidden within like some kind of a storage manager that would just speed us the data as we need it and under the hood, you understand that this there are like maybe there are indices maybe goes sequentially and. Fetches the data from the disk. It unpacks it. It finds the right position of the right. Attribute and so on. Now I wanna move our attention to overall query processing process. So query processing is a very complex process. And we will break it down step by step. On the right there is a schematic representation of of what's happening. So we are getting a query that is written in a high level language. Which has to be parsed and translated. Parsing and translation happens just as you in your regular compiler, right? So there is a grammar for the language you parse to the tokens. You have semantic analysis and so on and so forth. After that's done, you have what is essentially ease a DAC, right? A directional a cyclic graph with the relational algebra operators which we call to which we call a relational algebra expression. There are then various query optimizations and transformations happening inside the optimizer, so the optimizer takes the relational algebra expression, it changes it. It modifies it, but it preserves the semantics, of course, right? So the there are many equivalent class as we will see that we can look at and we still get the same result. The optimizer uses some statistics about the data and it produces what is called an execution plan. I'll show what it is in a bit, which is then submitted to the evaluation engine, which is actually accessing the data through the storage manager. It fetches data from the disk and then it applies the execution plan. When that's done, we materialize our output. And we show that to the user. Let's look at our example again. So I'm taking the very same query that we looked at. And let's look at how this query can be evaluated so. The main point here is that there are many ways of execute the same query. Since this one is pretty simple, there are not that many, but as your query gets. It's more and more complicated. The sheer number of. Ways to produce the correct result grows. So at the bottom here we see that there are two. Relational algebra expressions. And they are. Semantically identical. So notice the difference. The difference is that on the in the top one I am sorting the result of the two joins, right? So I first join. All the relations, student enrolment and courses, then I take the result and I apply a filter to it. And the second example, I first filter. The relation courses. And then I join it. The results of the filtering to other relations. This is an equivalent transformation and there is a number of such transformations available to the optimizer which it applies. Usually these are straightforward to make, like in this case it's probably clear that filtering something first is better than. First, joining the large tables and then filtering the results table. Just because if you join the large tables first then that means you probably need to allocate a lot of memory to store the temporary result. And if you don't do that, if you filter first, your temporary result would be much much much smaller. Is that clear? You see that these two plans are the same. Yes. Mm hmm. Here's an illustration of it right so. As a as a graph. As a DAG rather. Here are some more examples, like the transformations may happen not only at that level, but at multiple levels. So say we take a selection, right? So a selection is just like filtering a filtering query. So a selection may use different strategies to access the data, right? As we saw, we can just linear scan through all of the records and fetch everything from the disk. Or we could go through the index, right? There. Even more things to consider right there is data compression encoding. You can prefetch the data you can. You can have some data structures inside the database management systems that is shared between either queries or different part of queries. So we can share the scanned data like we fetch something from the disk we apply, say a filter and then we cache it. Then of course you can catch the result 'cause. You may have multiple queries that are doing the same thing. There is multithreading and more, more, more. The same story is with the algorithms. So if you look at the join, join can be calculated for example as a nested loop, right? You just go through all the records of the first relation and compare it to each and every record in in the second relation. Or you could build a hash map, right? You take the smallest relation. You create a hash partitioning. Basically a hash map. And then you go through the 2nd relation, you access the hash map and check if the the hashes hit. So the optimal choice depends on many, many factors, right? So the the the I think the most interesting one is data cardinality which will define and talk in a bit. But there are also the schema like how large are your records on how loaded is the system right? You may choose a different algorithm, or you may scale it to a different number of cores. Use. So much, so much so. Too many threads and so on. Or how much can you? Allocate for a given query right 'cause. There are multiple of those running at the same time. So when we specify how to run a query, we create. Or rather, when we specify. How we execute a single operator over relational algebra? What we get is called a valuation primitive, right? So it's just an annotated relational algebra expression. Here I have some examples right, so a sequence of such evaluation primitives. Calls a query execution plan. So the query execution plan is. What gets evaluated? So there are no. There are no unknowns about. These these graph so you can just take it and execute it. Of course you can also sort it right, so you can choose the order in which you traverse the computation. But that's it. Apart from that, what we will need further to discuss the optimizations in modern systems is the notion of a pipeline. If we take that very same relational algebra expression right, we may observe that there are certain properties about the path within. This plan. We may notice that some of the operators do not require intermediate materialization of the result. What does that mean? That means that if we have processed a single. Tuple by a single relational algebra expression. We may go ahead and pass it to the next one. There is no requirement to know about. All the results of the previous operator. And that's not always the case, right? So there are certain operators which we call a pipeline breaker that do require all the children to meet every, each and every tuple before the operator finishes. So here I'm listing some of the examples for the for such things. These are for example sub queries. It's joins always. Only on the build side though, so the I'm separating between the. Joins the build part and the probe part like like as you would do that when you build a hash table. And of course, whenever you do ordering, this is a pipeline pipeline Breaker 2. So if we look at the left part of the picture here, right? So going from courses to the end of the query, we can do that. In a single go, right? So if we assume that, let's say this join that we are doing on the way is done through a hash as a hash join. If we assume that the hash map has already. Been built then there is nothing preventing us from just going through the. Tuples 1 by 1 and executing all the operators in this pipeline. This essentially means from the optimization standpoint of view, that you could keep the tuples on the registers, right? And you can run all the functions you need directly on that memory, which significantly improves the performance obviously. This is I think this idea came around at about. 2011 org do you remember? About that time? Yeah, 1112. Mm HMM or to mm hmm. This is from a database called hyper. This was another idea and it came together with a technique for utilizing that which is a compilation just in time compilation, which we will also talk a little bit about. And essentially it allows you to save a lot of amount of data traffic between your main memory and the register file of a CPU. It's not specific to the CPU, of course, and this works for any type of computing device. OK, I'll, I'll. I'll make a short post here. Are there any questions here? We are getting to processing models. Mm hmm. OK. So this idea about keeping the tuples inside registers makes us think about how do we actually organize the evaluation of a plan, right? There are actually multiple ways to do that, but if you carefully think about it is not that many. So if I want to describe how my system processes the execution plan. I basically have to choose between these three options. The first option would be to not do as I just described and just materialise everything for a single primitive, right? So I have a filtering operator. So what I do is I'm just going. I'm taking my input relation. I'm going through each and every tuple if it fits the condition, I allocate memory and I put the result into. Some slot of that allocated memory. And once I'm done, I'm passing the control flow to the next operator. Simple enough. The opposite of that is the IT is called iterator model or volcano. This model is closer to what I have described previously, right? So we take a single TUPL and we process. We keep the the data in the register and we process it with a set of operators until we hit a pipeline breaker. So while we can, we keep the tuples inside the registers and if we cannot restart materializing. And the final option that we have is the middle ground. It's called vectorization or patched model. So it processes. It's basically the same iterate model, but instead of processing 2 + 1 by 1, we take them as batches. Of course, this comes at this is a trade off, right? So this comes at a price and with the benefit. So. I suppose the most common model is of course iterator, but new. New database systems are primarily based with the back transition in mind, because it enables the engine to utilize SIMD instructions, right? So you can have the your batch of. Tuples processed efficiently, especially if your database system stores the data as columns, which is called column store or column major store. Then your accesses through the memory are contiguous. This is very efficient and then you can directly run SIMD instructions on that memory. So when you have. The model established, right? So this gives you an opportunity to hide all the. Execution behind the execution plan. So you just. As a system, as a system designer. You've united your hands above the. Query execution plan. Right. So you just submit the query execution plan to the evaluator and you're done. So now in order to make the. Execution itself efficient. You'll have to look into it carefully. The query evaluation is. Basically, a classical optimization problem, right? So. When you look at it, you have. 2. Major options. So you have your execution plan. It says on the join must be done with. As a hash join. Now you want to somehow implement this. Hash join. So of course, if there are two options, one of them is to write a highly optimized algorithm. And ship it as a binary with together with the database system. So it would be very specific, right? You specialized it for a specific target hardware in mind. Well, you could not do that, but then you'll sacrifice some performance. So you'd probably do that. You are. Then restricted to the set of operators that you have implemented, right? So the number the options that the optimizer would have would depend on the number of things you have implemented. If you've only implemented has join, then the OPTIMISER would do nothing in that terms, right? So it could not optimize joins. More than you've provided it with. The the highly optimized handwritten algorithm also have low overheads. Right, because you don't. But you don't spend any time when running them. You just set a pointer and start the code, but it has a downside to where you have to actually call it right? So if you have a chain of operators and they're like a pipeline and they are not implemented as a single function, then you'll have to pay the penalty of actually going and calling each and every method. That implements an operator. Yep, that gives you an opportunity of easily predictable performance characteristics. Another thing you could do is you could on the fly compile. The some kernel that would run your computation. Exactly as you need it. So this would use some generic tool chain like some kind of a compiler. And it supports anything you want as long as you've written the path to convert the graphic. The computational graph, which in this case is. Query execution plan to the machine code. Well, not machine code. Intermediate representation of the compiler. You pay the price of actually doing that, right? So you have to. Lower the graph to intermediate representation. You have to run the optimization process. You have to run the back end. Choose the instruction. Do the register location and so on, so you will pay the latency of compiling the query first. But there are no calls, right? So if you have a long pipeline, you don't have to allocate the stack. Move it. You just go with the with a single. Nice and easy generated code. Everything is in light. And of course you got you have the downside of. All the complexities that come with the generated code it's a bit hard to debug. You'll need instrumentation to verify it. The performance is not predictable, which is important. Yet it can produce very nice results in certain cases. There is no single solution for all the database management systems there. Both approaches exist. Moreover, they coexist. And I think it would be fair to say that. They always coexist. Usually people somehow implement very specific algorithms for very specific things. Like, say very specific join types. And in any case, you want some kind of code generation for complex filters, and so if you ask I want to query the names that are these like this age they attend this course and also they've attended this course and so on. Your like list of. Condition conditions conditions grow. And if you just blindly. Run. And implement a single like operator implementation. You would end up with a huge amount of calls, right? So this is usually called generated. The the next step we look at is query optimization, right? So. As we mentioned earlier and we saw that query plans may have different associated costs, right? So exactly like that trade off, we've looked, we've just looked at, we could have, we could potentially have, say two implementations of a join. One of them is written in C++. We optimize it for a specific thing. We use a specific hash function. And we. I don't know use a specific allocator. Now, that's not our only option. We can also generate some code and include the filters inside the code that does the that forms the join so that the pipeline that we have. Fits into a single. Query like pipeline invocation. So there are two our two options. It's the job of the optimizer to pick the. Most performing. Physical plan. So I'm using slightly different names here, right? But there are the same. So the logical plan is our input relational algebra expression and the physical plan is that same. Semantically same. Graph but annotated. So how does? First of all, the the target of the optimization is of course the plan and it tries to produce the best plan as specified by the database management system. The meaning of the best may vary, right? So it could be the query response latency, it could be the number of disk accesses. It could be the cost. So in dollars. Something and it's usually multiple things. So it's a multi criteria optimization problem which are usually pretty hard. In fact this is an NP hard problem. I'm not quite sure if it's proven exactly, but this is definitely. An NP hard problem. This is at the core of the promise of relational algebra. This is what drives the. Requirement of users not knowing about the physical layout of the data. And the database management system internals. If the optimizer cannot do its job, then the whole model falls apart. And since at each core there is ANP heart problem, this is a complex issue to solve. So there is a lot of research done into in this area over the course of how many like 70 years. The number of valid query plans grows exponentially as you implement a new operator or a new algorithm. For an operator, you add complexity, right? You add basically. An another branch of choice for the query optimizer. The number of such possibilities. May be impossible to look at, even just to consider. Right, not like run them and test them and see what's better. So there are different scenarios in which. You can. You can do and you cannot do that. When you can do that is usually something like. There is a database and you run recurring queries to that database system. That can be like. If your database is connected to some, I don't know web form. You're building Amazon and when the user buys something, there is a certain query that is running. Its would be usually the same right? And you could. You you can spend a lot of time trying to optimize that. There are other cases where you can't do that right. This is the prerogative, for example, of exploratory. Data analysis where the the query is definitely unknown before it's executed. So there this problem shines. So in order to pick. Between the options, the optimizer must somehow. Navigate this the search space it is doing so by cost modeling. So early cost modeling is based on known hardware parameters since the early database systems were built. By just implementing the operators and as we've seen, this is like easy to predict. And estimate they have easy to predict and performance characteristics. So in order to estimate the cost of plan, what the optimizer can do, it can just traverse the. Deck of the queries execution plan. And just calculate the number of. CPU cycles that are necessary to process the data. That holds if and only if you know how much data you wanna process. Unfortunately, that is not that is not always true. Umm. You don't since you have selection operators, right? You you don't know how much data goes through that operator. Just because you don't. Know what portion of the data. Satisfies the predicate that you're evaluating. This is a serious problem and cost modeling techniques account for that. This is called cardinality estimation. There are. Well, there are probably seven major ways of doing the cardinality estimation. This is what was. Drawn on the picture of the optimizer that it accesses some metadata. Yeah, that's exactly that. Usually how it's done is a database management system would while it runs some queries it records some additional data to a separate storage. That tells you for this relation the number of, let's say. Hours in our example, students we have cohorts of students that are like, say. Less than 30 years old. And we can like have proportions of such. Students loan before running a queries somewhat known because when you when we change the relations, of course these estimations drift. But this is the primary way of collecting that estimating the cost of the complete query. There are other ways of course, to do that, and they are being explored actively and they are used. Like learning based methods and they are applied to both cardinality estimation and on the cost US estimation overall. With different success rates. But this is used. All right, parallelism all right. How many time do I have, you know? We are running all the time. Right after time. Yeah, that's unfortunate. Yeah. Yeah, I just can quickly go over. Not let's rock up. OK. What mm hmm. I'll I'll just mention briefly some stuff and then wrap it up. So of course the database management system must be parallel cause more than architectures are parallel and we have different levels of parallelism occurring in the system. So this requires concurrent accesses and the database management systems guarantees. Concurrent accesses safety by creating algorithms that are concurrency safe. So if I'm running. Two my queries. One of them inserts a tuple and another one reads. The I'm. Allow I'm creating algorithms that access the indexes and other data structures that are thread safe. Concurrency safe? At a high level, the parallelism. The design of the database may have different properties. There are certain ways of organizing the database systems. So these are the main ones. So you may have a shared memory between the nodes when where the processes can access the data directly and act on it together. There are shared disk systems which are also referred as clusters sometimes. When there are the memory is separate for each processor, but the disk the disks are not. And there is shared nothing when there is no resources are shared or you can have. On a combination of these, in some hierarchical structure. The query parallelism that you. Exploit comes from two sources you can have. The parallelism within a single query where you either implement an algorithm of a hash join that is run on multiple cores, or you could run independent pipelines within a single query. And of course you can run multiple. Queries at the same time on from a different users, for example. They're not exclusive. Mutually exclusive. Each database system utilizes both and. Here I have some examples of these. You can read them up later. When the slides are posted, oh, they actually are already posted. And here I'll just briefly mention the target scenarios and I think I'll wrap up so. When you look at the. Ways the parallelism is exploited. You may notice that there are certain differences in how the data and the data structures are organized, and that is because due to the two primary ways of using a database. So one of them is the called online transaction processing or LTP. This is the classical approach of using the database system where you just run your data day-to-day business activities. This is very sensitive to latencies usually. Think of trying to buy something from Amazon. The and the other. Way of using DPS is called online analytical processing. This is completely different. It's a small amount of long running queries. They're usually read only and they are. They are trying to gather some data and perform some analysis on for decision making. Of course. So you wanna use both for each and every use case, like an enterprise? But yet they have contradictory optimization challenges. Which is why people build different solutions for different types of workloads and organize the enterprise into complex systems. That consists of like a whole CP database for some online activities. Then you build in some kind of a warehouse or a data leak. And there is a process that is called ETL and. Stands for extract, transform and load that populates your data warehouse. With the actual data for decision making. So this is basically data integration process that. That includes data search. In enterprise there are different multiple sources, multiple systems services that you gather the data from. They are usually distributed in time and space and have different versions, so it's a very complex problem. Then you have to read that data. It's distributed, it's sharded over and then the network. Then there is usually some preprocessing steps. And finally you have to do the data analysis with some kind of algorithms and then report. All right, I'm done. And I think I'm just on time. Yep, just one time. And yeah, thank you, Peter. Very much. This was a very, very condensed overview of the whole. Domain in just 90 minutes, so please forgive but this was very important because in the last lecture what we did is we started with a problem which was how to search and rank the web pages and the central element there. Was it algorithm called Peter Rank and we started with it algorithm. Then we start designing a system and used a system design as a algorithm delivering vehicle. Here we went even higher. Practically what Peter did, he showed that. When you start with with a problem. Of how to. How to store data and how do query data? It becomes a big thing in itself. It's a huge domain of. Research in computer science and. What we did, we split it into like 4 main domains. 4 main parts. First there was a language. Which or as we called it, calcos said. Hey, there's a calculus which is expressing this relational model very good and relational model is one of the approaches to do this, but most importantly. To effectively implement this language, what we need, we need a translator. Or compiler high level compiler from language or calculus to the underlying algebra and the algebra becomes central alman or say hey doesn't matter what type of language you have on the top, it doesn't. They can be translated and expressed in this algebra. We can do the following thing and then the further thing was to. Stop refinement. One from algebra, you create a logical query plan. Which assumes data and process data in a very logical way. Here is my relations. Here are the operators or algebraic operators. Here is how I'm going to optimize them. To minimize some data movement etc. But most importantly, what Peter did, he showed that how you. That going. How you go from the logical query plan to the physical query plan and physical query execution. And that is the most hard, difficult. And research worthy part on this domain because. The time and the real real life car kicks in or is the same US under the rubber hits the road, things become really can become really bad and you need to really understand what is the hardware we're running on, what capabilities does it have? What is my data? What are the data retrieval times? How data is organized? What are the queries? How I'm going to optimize them to get? The best performance and that is very hot. So in this lecture we tried to explain how you go from the top language to the bottom query execution using various mechanism and the last one was the last step was query compilation and parallelism exploitation, which itself is a huge domain. But we hope that we showed some of the problems and how these problems can be tackled. Any questions so far? Any anything that? Remain very unclear. Part of this unclear, but anything. Is there anything that very unclear? All know we were cut. No, I don't think so. Yes. Tomorrow you said you were saying. No, I was just saying that we were kind of familiar, no questions so. OK. So, OK, let me stop recording.